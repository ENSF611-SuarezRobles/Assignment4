{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for text processing tasks\n",
    "!pip install contractions > /dev/null\n",
    "import contractions\n",
    "import nltk # Imports the Natural Language Toolkit module\n",
    "import re # regex\n",
    "\n",
    "# Imports for pipelines, modeling, and evaluation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dataset (1 mark)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# I selected 3 categories that I like out of the 20 because every run was taking a loooooong time\n",
    "# Note that because I am still using 3 categories, this is a multiclass classification problem\n",
    "selected_categories = [\n",
    "    'sci.electronics',   # Topics related to electronics\n",
    "    'sci.med',           # Topics related to medical sciences\n",
    "    'sci.space',         # Topics related to space science \n",
    "]\n",
    "\n",
    "# Fetch only data for the selected topics\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=selected_categories)\n",
    "posts, targets = [s.strip() for s in newsgroups.data], newsgroups.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df26345d-1fa3-4ef0-a137-a71100615bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources to process Text Data                   \n",
    "_ = nltk.download('punkt', quiet=True)           # Downloads a pre-trained tokenizer models used to split sentences\n",
    "_ = nltk.download('stopwords', quiet=True)       # Downloads a set of stopwords\n",
    "_ = nltk.download('wordnet', quiet=True)         # Downloads a lexical database of English, used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc244d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean data (if needed)\n",
    "\n",
    "# Custom transformer for word expansion\n",
    "class WordExpander(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [contractions.fix(text) for text in X]\n",
    "\n",
    "# Custom transformer for text cleaning\n",
    "class TextCleaning(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [re.sub(r'[^\\w\\s]', '', text.replace(\"'s\", \"\")) for text in X]\n",
    "\n",
    "# Custom transformer for tokenization\n",
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [nltk.word_tokenize(text) for text in X]\n",
    "\n",
    "# Custom transformer for stop word removal\n",
    "class StopwordRemover(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        return [[word for word in tokens if word.lower() not in stop_words] for tokens in X]\n",
    "\n",
    "# Custom transformer for lemmatization\n",
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        return [' '.join([lemmatizer.lemmatize(word) for word in tokens]) for tokens in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a8c127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12972f2c-c06f-4fa9-8d0e-f3d6a890704d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports for supervised learning classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(posts, targets, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5558a776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Estimator: Pipeline(steps=[('word_expander', WordExpander()),\n",
      "                ('text_cleaning', TextCleaning()), ('tokenizer', Tokenizer()),\n",
      "                ('stopword_remover', StopwordRemover()),\n",
      "                ('lemmatizer', Lemmatizer()),\n",
      "                ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=10, max_iter=5000, solver='liblinear'))])\n",
      "Best Overall Parameters: {'classifier': LogisticRegression(C=10, max_iter=5000, solver='liblinear'), 'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "\n",
      " Cross-Validation accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "\n",
    "# Define scoring metrics for all classifiers\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_score': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Create a general pipeline with a placeholder for the classifier\n",
    "pipeline = Pipeline([\n",
    "    ('word_expander', WordExpander()),\n",
    "    ('text_cleaning', TextCleaning()),\n",
    "    ('tokenizer', Tokenizer()),\n",
    "    ('stopword_remover', StopwordRemover()),\n",
    "    ('lemmatizer', Lemmatizer()),\n",
    "    ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
    "    ('classifier', None)  # Placeholder for the classifier\n",
    "])\n",
    "\n",
    "# Define a combined parameter grid\n",
    "param_grid = [\n",
    "    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "     'classifier__C': [0.1, 1, 10],\n",
    "     'classifier__penalty': ['l1', 'l2'],\n",
    "     'classifier__solver': ['liblinear', 'saga']},\n",
    "    {'classifier': [MultinomialNB()],\n",
    "     'classifier__alpha': [0.1, 1, 10]},\n",
    "    {'classifier': [KNeighborsClassifier()],\n",
    "     'classifier__n_neighbors': [3, 5, 7],\n",
    "     'classifier__weights': ['uniform', 'distance']}\n",
    "]\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='f1_score')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best model parameters and results\n",
    "print(\"Best Overall Estimator:\", grid_search.best_estimator_)\n",
    "print(\"Best Overall Parameters:\", grid_search.best_params_)\n",
    "print(f'\\n Cross-Validation F1 Score: {grid_search.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38aceea-0d61-4e80-b138-7b7ff234b0e9",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "print(f'Test accuracy {grid_search.score(X_test, y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
